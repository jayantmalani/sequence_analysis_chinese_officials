{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cost_deletion = 3\n",
    "cost_insertion = 2\n",
    "cost_substitution = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costDeletion(s):\n",
    "    return int(3)\n",
    "\n",
    "def costInsertion(s):\n",
    "    return int(3)\n",
    "\n",
    "def costSubstitution(s1, s2):\n",
    "    return int(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OptimalMatching(s1, s2):\n",
    "    assert isinstance(cost_deletion, int)\n",
    "    assert isinstance(cost_insertion, int)\n",
    "    assert isinstance(cost_substitution, int)\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    cost = np.zeros((lenstr1 + 1, lenstr2 + 1), dtype=int)\n",
    "    \n",
    "    # initialization : comparaison with a null sequence\n",
    "    min_cost = min(cost_deletion, cost_insertion)\n",
    "    for i in range(lenstr1 + 1):\n",
    "        cost[i, 0] = i*min_cost\n",
    "    for j in range(lenstr2 + 1):\n",
    "        cost[0, j] = j*min_cost\n",
    "\n",
    "    for el1 in range(lenstr1):\n",
    "        for el2 in range(lenstr2):\n",
    "            if s1[el1] == s2[el2]:\n",
    "                cost[el1 + 1, el2 + 1] = cost[el1, el2] # cost = 0 because they are the same\n",
    "            else:\n",
    "                cost[el1 + 1, el2 + 1] = min(\n",
    "                                           cost[el1, el2 + 1] + costDeletion(s2[el2]), # deletion\n",
    "                                           cost[el1 + 1, el2] + costInsertion(s1[el1]), # insertion\n",
    "                                           cost[el1, el2] + costSubstitution(s1[el1],s2[el2]) # substitution\n",
    "                                          )\n",
    "    return cost[lenstr1, lenstr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SequenceAnalysis(careers):\n",
    "    maxLen = len(careers)\n",
    "    costTable = np.zeros((maxLen, maxLen), dtype=int)\n",
    "    for i in range(len(careers)):\n",
    "        for j in range(len(careers)):\n",
    "            costTable[i][j] = OptimalMatching(careers[i],careers[j])\n",
    "    return costTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "seq1 = [\"job1\",\"job2\",\"job3\"]\n",
    "seq2 = [\"job1\",\"job3\"]\n",
    "\n",
    "print(OptimalMatching(seq1, seq2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 4],\n",
       "       [3, 0, 1],\n",
       "       [4, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [[\"job1\",\"job2\",\"job3\"],[\"job1\",\"job3\"],[\"job1\",\"job6\"]]\n",
    "\n",
    "table = SequenceAnalysis(list)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as ssd\n",
    "import scipy as sp\n",
    "import sklearn as ap\n",
    "distArray = ssd.squareform(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bb63e103eb76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'cluster'"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-752f344eef16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhierarchy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'centroid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'cluster'"
     ]
    }
   ],
   "source": [
    "clusters = sp.cluster.hierarchy.linkage(distArray, method='centroid', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-d21502973a35>, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-d21502973a35>\"\u001b[1;36m, line \u001b[1;32m83\u001b[0m\n\u001b[1;33m    newsim = 0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def count_cluster_sizes(no_clusters, no_docs, r):\n",
    "    cluster_sizes = [0 for x in range(no_clusters)]\n",
    "    for j in range(no_clusters):\n",
    "        for i in range(no_docs):\n",
    "            if r[i] == j:\n",
    "                cluster_sizes[j] += 1\n",
    "    return cluster_sizes\n",
    "\n",
    "\n",
    "def initialize_clusters(x, cluster_ids, doc_ids):\n",
    "\n",
    "    no_docs = len(doc_ids)\n",
    "    no_clusters = len(cluster_ids)\n",
    "\n",
    "    # Initialize shuffled docs ids.\n",
    "    shuffled_docs = np.array(doc_ids)\n",
    "    np.random.shuffle(shuffled_docs)\n",
    "\n",
    "    # Initialize new assigments.\n",
    "    r_new = np.zeros(no_docs, np.int32)\n",
    "\n",
    "    # Initialize the centroids.\n",
    "    mu = np.copy(x)[shuffled_docs[:no_clusters],:]\n",
    "\n",
    "    return shuffled_docs, r_new, mu\n",
    "\n",
    "\n",
    "def spkmeans(x, no_clusters, verbose=False, **kwargs):\n",
    "\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        x = x.toarray()\n",
    "\n",
    "    # Get doc and cluster ids.\n",
    "    no_docs = x.shape[0]\n",
    "    cluster_ids = range(no_clusters)\n",
    "    doc_ids = range(no_docs)\n",
    "\n",
    "    on_empty = kwargs.get('on_empty','restart')\n",
    "\n",
    "    # Initialize the clusters\n",
    "    shuffled_docs, r_new, mu = initialize_clusters(x, cluster_ids, doc_ids)\n",
    "\n",
    "    # Initialize count and similary array.\n",
    "    count = 0\n",
    "    similarity = []\n",
    "\n",
    "    # Spherical k means loop.\n",
    "    while True:\n",
    "\n",
    "        # Iteration start time.\n",
    "        startime = time.time()\n",
    "\n",
    "        # Update assignments.\n",
    "        # Copy the old assignments.\n",
    "        r = np.copy(r_new)\n",
    "\n",
    "        # Compute the new assignments.\n",
    "        products = np.dot(x,mu.T)\n",
    "        r_new = np.argmax(products,axis=1)\n",
    "\n",
    "        # Collect and sort the new scores.\n",
    "        scores = np.array([products[i, r_new[i]] for i in doc_ids])\n",
    "        scores_idx = np.argsort(scores)\n",
    "\n",
    "        # Fix empty clusters here.\n",
    "        empty = [i for i in cluster_ids if i not in r_new]\n",
    "        if empty:\n",
    "            #print 'Iteration %i: empty clusters: %s' % (count, str(empty))\n",
    "            if on_empty == 'restart':\n",
    "                #print 'Reinitializing algorithm.'\n",
    "                shuffled_docs, r_new, mu = initialize_clusters(x, cluster_ids,\n",
    "                                                               doc_ids)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                #print 'Reassinging remote data to empty clusters.'\n",
    "                for i,j in enumerate(empty):\n",
    "                    r_new[scores_idx[-(i+1)]] = j\n",
    "                empty = [i for i in cluster_ids if i not in r_new]\n",
    "                if verbose:\n",
    "                    #print 'Adjusted empty clusers: ' + str(empty)\n",
    "\n",
    "        newsim = 0\n",
    "        for i in range(no_docs):\n",
    "            newsim += products[i,r_new[i]]\n",
    "        similarity.append(newsim)\n",
    "\n",
    "        # Exit if assigments do not change.\n",
    "        if np.all(r == r_new):\n",
    "            sizes = count_cluster_sizes(no_clusters, no_docs, r)\n",
    "            return mu, r, similarity, sizes\n",
    "\n",
    "        # Update centroids.\n",
    "        mu = np.zeros_like(mu)\n",
    "        for i in range(no_docs):\n",
    "            mu[r_new[i],:] += x[i,:]\n",
    "        for j in range(no_clusters):\n",
    "            mu_norm = np.linalg.norm(mu[j,:])\n",
    "            if mu_norm == 0.0:\n",
    "                print 'WARNING: Cluster %i empty!' % j\n",
    "\n",
    "            else:\n",
    "                mu[j,:] = mu[j,:]/mu_norm\n",
    "\n",
    "        delta = time.time() - startime\n",
    "        if verbose:\n",
    "            print 'Iteration %i: %.2f seconds.' % (count, delta)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hpc_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fd318ccdf827>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhpc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhpc_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mk_means\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mk_means\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhpc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hpc_fit' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hpc = PCA(n_components=2).fit_transform(hpc_fit)\n",
    "k_means = KMeans()\n",
    "k_means.fit(hpc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
